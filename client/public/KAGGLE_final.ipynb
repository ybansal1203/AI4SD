{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmpNE9UvgArF",
        "outputId": "91c3d3d9-23fa-4d1d-8c48-75846ea711c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# 0. 기본 셋업 & 패키지 설치\n",
        "# =============================\n",
        "!pip install timm iterative-stratification kagglehub grad-cam -q\n",
        "\n",
        "import os, json, time, random, math, glob\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as tvm\n",
        "\n",
        "import torchvision.transforms as T\n",
        "import timm\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import kagglehub\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWpcGJMeiCdN",
        "outputId": "9bec2e91-0b69-47d4-d1e4-11c0fe4c7ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE_DIR: /root/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3\n",
            "Num classes: 15\n",
            "Labels: ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# 1. 데이터 다운로드 및 기본 메타 구성\n",
        "# =============================\n",
        "\n",
        "path = kagglehub.dataset_download(\"nih-chest-xrays/data\")\n",
        "\n",
        "# KaggleHub로 NIH Chest X-ray 다운로드\n",
        "BASE_DIR = \"/root/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3\"\n",
        "print(\"BASE_DIR:\", BASE_DIR)\n",
        "\n",
        "LABEL_CSV = os.path.join(BASE_DIR, \"Data_Entry_2017.csv\")\n",
        "TRAIN_TXT = os.path.join(BASE_DIR, \"train_val_list.txt\")\n",
        "TEST_TXT  = os.path.join(BASE_DIR, \"test_list.txt\")\n",
        "\n",
        "df_meta = pd.read_csv(LABEL_CSV)\n",
        "\n",
        "def parse_labels(s):\n",
        "    parts = [p.strip() for p in s.split('|')]\n",
        "    return [p for p in parts if p != '']\n",
        "\n",
        "df_meta['labels_list'] = df_meta['Finding Labels'].apply(parse_labels)\n",
        "\n",
        "all_labels = sorted({l for labels in df_meta['labels_list'] for l in labels})\n",
        "label2idx = {l: i for i, l in enumerate(all_labels)}\n",
        "idx2label = {i: l for l, i in label2idx.items()}\n",
        "num_classes = len(all_labels)\n",
        "\n",
        "print(\"Num classes:\", num_classes)\n",
        "print(\"Labels:\", all_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "WW5lX0LtiFPI",
        "outputId": "c893dcf9-d70f-4974-f206-bf69d1310ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total image files found: 112120\n",
            "Train+Val files: 86524\n",
            "Test files: 25596\n",
            "(86524, 4) (25596, 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_train_val_all\",\n  \"rows\": 86524,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 86524,\n        \"samples\": [\n          \"00028324_001.png\",\n          \"00006949_004.png\",\n          \"00003923_000.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 86524,\n        \"samples\": [\n          \"/root/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3/images_012/images/00028324_001.png\",\n          \"/root/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3/images_004/images/00006949_004.png\",\n          \"/root/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3/images_002/images/00003923_000.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_train_val_all"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e47ae535-8285-472f-bc7b-2b2795031974\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>path</th>\n",
              "      <th>labels_list</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000001_000.png</td>\n",
              "      <td>/root/.cache/kagglehub/datasets/nih-chest-xray...</td>\n",
              "      <td>[Cardiomegaly]</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00000001_001.png</td>\n",
              "      <td>/root/.cache/kagglehub/datasets/nih-chest-xray...</td>\n",
              "      <td>[Cardiomegaly, Emphysema]</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00000001_002.png</td>\n",
              "      <td>/root/.cache/kagglehub/datasets/nih-chest-xray...</td>\n",
              "      <td>[Cardiomegaly, Effusion]</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00000002_000.png</td>\n",
              "      <td>/root/.cache/kagglehub/datasets/nih-chest-xray...</td>\n",
              "      <td>[No Finding]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00000004_000.png</td>\n",
              "      <td>/root/.cache/kagglehub/datasets/nih-chest-xray...</td>\n",
              "      <td>[Mass, Nodule]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e47ae535-8285-472f-bc7b-2b2795031974')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e47ae535-8285-472f-bc7b-2b2795031974 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e47ae535-8285-472f-bc7b-2b2795031974');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-35c1a1da-5e44-4f90-a6a7-fe34c504ceee\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35c1a1da-5e44-4f90-a6a7-fe34c504ceee')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-35c1a1da-5e44-4f90-a6a7-fe34c504ceee button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           filename                                               path  \\\n",
              "0  00000001_000.png  /root/.cache/kagglehub/datasets/nih-chest-xray...   \n",
              "1  00000001_001.png  /root/.cache/kagglehub/datasets/nih-chest-xray...   \n",
              "2  00000001_002.png  /root/.cache/kagglehub/datasets/nih-chest-xray...   \n",
              "3  00000002_000.png  /root/.cache/kagglehub/datasets/nih-chest-xray...   \n",
              "4  00000004_000.png  /root/.cache/kagglehub/datasets/nih-chest-xray...   \n",
              "\n",
              "                 labels_list  \\\n",
              "0             [Cardiomegaly]   \n",
              "1  [Cardiomegaly, Emphysema]   \n",
              "2   [Cardiomegaly, Effusion]   \n",
              "3               [No Finding]   \n",
              "4             [Mass, Nodule]   \n",
              "\n",
              "                                                   y  \n",
              "0  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "1  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
              "2  [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =============================\n",
        "# 1-1. 이미지 경로 매핑 (파일명 → full path)\n",
        "# NIH Chest 데이터는 images_001/... 등의 폴더 안에 존재\n",
        "# =============================\n",
        "\n",
        "image_paths = glob.glob(os.path.join(BASE_DIR, \"images_*\", \"images\", \"*\"))\n",
        "print(\"Total image files found:\", len(image_paths))\n",
        "\n",
        "fname2path = {os.path.basename(p): p for p in image_paths}\n",
        "list(fname2path.items())[:5]\n",
        "\n",
        "# =============================\n",
        "# 1-2. train/test 리스트 읽기\n",
        "# =============================\n",
        "\n",
        "def read_list_file(path):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = [ln.strip() for ln in f.readlines()]\n",
        "    files = [os.path.basename(ln) for ln in lines if ln.strip()]\n",
        "    return files\n",
        "\n",
        "train_val_files = read_list_file(TRAIN_TXT)  # 86524 개\n",
        "test_files      = read_list_file(TEST_TXT)   # 25596 개\n",
        "\n",
        "print(\"Train+Val files:\", len(train_val_files))\n",
        "print(\"Test files:\", len(test_files))\n",
        "\n",
        "meta_subset = df_meta[['Image Index', 'labels_list']].copy()\n",
        "meta_subset.rename(columns={'Image Index': 'filename'}, inplace=True)\n",
        "fname2labels = dict(zip(meta_subset['filename'], meta_subset['labels_list']))\n",
        "\n",
        "# =============================\n",
        "# 1-3. 파일 목록과 메타데이터 조인\n",
        "# =============================\n",
        "\n",
        "def labels_to_multihot(labels):\n",
        "    vec = np.zeros(num_classes, dtype=np.float32)\n",
        "    for l in labels:\n",
        "        if l in label2idx:\n",
        "            vec[label2idx[l]] = 1.0\n",
        "    return vec\n",
        "\n",
        "def make_df_from_files(file_list):\n",
        "    rows = []\n",
        "    for fn in file_list:\n",
        "        if fn not in fname2labels:\n",
        "            continue\n",
        "        if fn not in fname2path:\n",
        "            continue\n",
        "        labels = fname2labels[fn]\n",
        "        rows.append({\n",
        "            \"filename\": fn,\n",
        "            \"path\": fname2path[fn],\n",
        "            \"labels_list\": labels,\n",
        "            \"y\": labels_to_multihot(labels)\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_train_val_all = make_df_from_files(train_val_files)\n",
        "df_test_all      = make_df_from_files(test_files)\n",
        "\n",
        "print(df_train_val_all.shape, df_test_all.shape)\n",
        "df_train_val_all.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Suu6mrviJPi",
        "outputId": "e4312bc7-e98a-40c4-c899-b6f451ac4b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading existing split: /root/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3/split_train_val_30_10.csv\n",
            "split\n",
            "unused    51918\n",
            "train     25974\n",
            "val        8632\n",
            "Name: count, dtype: int64\n",
            "Train size: 25974\n",
            "Val size: 8632\n",
            "Test size: 25596\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# 2. Multilabel stratified split (train 30%, val 10% of original train)\n",
        "# 한 번 분할한 결과를 CSV로 저장/로드\n",
        "# =============================\n",
        "\n",
        "split_csv_path = os.path.join(BASE_DIR, \"split_train_val_30_10.csv\")\n",
        "\n",
        "if os.path.exists(split_csv_path):\n",
        "    print(\"Loading existing split:\", split_csv_path)\n",
        "    df_split = pd.read_csv(split_csv_path)\n",
        "else:\n",
        "    print(\"Creating new stratified split...\")\n",
        "    N = len(df_train_val_all)\n",
        "    X_dummy = np.zeros((N, 1))\n",
        "    Y       = np.stack(df_train_val_all['y'].values)\n",
        "\n",
        "    indices = np.arange(N)\n",
        "\n",
        "    # 1) 먼저 전체의 30%를 train, 나머지 70%를 remaining으로\n",
        "    # test_size=None으로 설정하여 나머지로 자동 계산되게 함 (rounding issue 방지)\n",
        "    msss1 = MultilabelStratifiedShuffleSplit(\n",
        "        n_splits=1, train_size=0.3, test_size=None, random_state=42\n",
        "    )\n",
        "    train_idx, rem_idx = next(msss1.split(X_dummy, Y))\n",
        "\n",
        "    # 2) remaining(70%)에서 다시 val 10% / 나머지 60%\n",
        "    Y_rem = Y[rem_idx]\n",
        "    X_dummy_rem = np.zeros((len(rem_idx), 1))\n",
        "\n",
        "    # val 비율은 전체 대비 10%이므로, remaining 내에서는 10 / 70 ≈ 0.142857\n",
        "    val_ratio_in_rem = 10.0 / 70.0\n",
        "\n",
        "    # test_size=None으로 설정하여 rounding issue 방지\n",
        "    msss2 = MultilabelStratifiedShuffleSplit(\n",
        "        n_splits=1, train_size=val_ratio_in_rem, test_size=None,\n",
        "        random_state=43\n",
        "    )\n",
        "    val_sub_idx, _ = next(msss2.split(X_dummy_rem, Y_rem))\n",
        "    val_idx = rem_idx[val_sub_idx]\n",
        "\n",
        "    split_labels = np.array(['unused'] * N, dtype=object)\n",
        "    split_labels[train_idx] = 'train'\n",
        "    split_labels[val_idx]   = 'val'\n",
        "\n",
        "    df_split = df_train_val_all[['filename']].copy()\n",
        "    df_split['split'] = split_labels\n",
        "    df_split.to_csv(split_csv_path, index=False)\n",
        "    print(\"Saved split to:\", split_csv_path)\n",
        "\n",
        "# df_split: filename, split\n",
        "print(df_split['split'].value_counts())\n",
        "\n",
        "# =============================\n",
        "# 2-1. split 정보 병합해서 최종 train/val/test DF 구성\n",
        "# =============================\n",
        "\n",
        "df_train_val_all = df_train_val_all.merge(df_split, on='filename', how='left')\n",
        "df_train = df_train_val_all[df_train_val_all['split'] == 'train'].reset_index(drop=True)\n",
        "df_val   = df_train_val_all[df_train_val_all['split'] == 'val'].reset_index(drop=True)\n",
        "\n",
        "# test는 전체 test_list 그대로 사용\n",
        "df_test = df_test_all.reset_index(drop=True)\n",
        "\n",
        "print(\"Train size:\", len(df_train))\n",
        "print(\"Val size:\", len(df_val))\n",
        "print(\"Test size:\", len(df_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXw2m61piM_1",
        "outputId": "9d55f4eb-a60c-4841-e28f-7de2382f074e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(203, 68, 200)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =============================\n",
        "# 3. Dataset & DataLoader\n",
        "# =============================\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "train_tfms = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_tfms = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class MultiLabelDataset(Dataset):\n",
        "    def __init__(self, df, transforms=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row['path']).convert('RGB')\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        y = torch.from_numpy(row['y'])\n",
        "        return img, y\n",
        "\n",
        "train_ds = MultiLabelDataset(df_train, transforms=train_tfms)\n",
        "val_ds   = MultiLabelDataset(df_val,   transforms=val_tfms)\n",
        "test_ds  = MultiLabelDataset(df_test,  transforms=val_tfms)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "len(train_loader), len(val_loader), len(test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLDc45xDiQWB"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# 4. 모델 생성 (Backbone freeze + 마지막 layer만 학습)\n",
        "# =============================\n",
        "\n",
        "def create_backbone(model_name, pretrained=True):\n",
        "    \"\"\"\n",
        "    model_name: 'vit', 'resnet50', 'efficientnet'\n",
        "    torchvision의 ImageNet pretrained 모델 사용.\n",
        "    backbone의 마지막 FC/classifier는 Identity로 바꾸고,\n",
        "    그 전의 feature dimension을 in_features로 리턴.\n",
        "    \"\"\"\n",
        "    if model_name == 'resnet50':\n",
        "        weights = tvm.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        backbone = tvm.resnet50(weights=weights)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Identity()\n",
        "\n",
        "    elif model_name == 'efficientnet':\n",
        "        weights = tvm.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        backbone = tvm.efficientnet_b0(weights=weights)\n",
        "        # classifier는 [Dropout, Linear] 구조\n",
        "        in_features = backbone.classifier[1].in_features\n",
        "        backbone.classifier = nn.Identity()\n",
        "\n",
        "    elif model_name == 'vit':\n",
        "        weights = tvm.ViT_B_16_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        backbone = tvm.vit_b_16(weights=weights)\n",
        "        in_features = backbone.heads.head.in_features\n",
        "        backbone.heads.head = nn.Identity()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
        "\n",
        "    # backbone freeze\n",
        "    for p in backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    return backbone, in_features\n",
        "\n",
        "\n",
        "class LinearHeadModel(nn.Module):\n",
        "    def __init__(self, backbone, in_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.classifier = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)          # (B, in_features)\n",
        "        logits = self.classifier(feats)\n",
        "        return logits\n",
        "\n",
        "# supervised contrastive용: projection head까지 포함한 embedding 모델\n",
        "class SupConModel(nn.Module):\n",
        "    def __init__(self, backbone, in_features, proj_dim=128):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.proj = nn.Linear(in_features, proj_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        z = self.proj(feats)\n",
        "        z = nn.functional.normalize(z, dim=1)\n",
        "        return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP0Hn6UeiS-y"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# 5. Metric 계산 함수 (6개 지표)\n",
        "# =============================\n",
        "\n",
        "def compute_metrics(logits, targets, threshold=0.5):\n",
        "    probs = 1 / (1 + np.exp(-logits))  # sigmoid\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    metrics['f1_micro'] = f1_score(targets, preds, average='micro', zero_division=0)\n",
        "    metrics['f1_macro'] = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "\n",
        "    try:\n",
        "        auc_macro = roc_auc_score(targets, probs, average='macro')\n",
        "    except ValueError:\n",
        "        auc_macro = np.nan\n",
        "    metrics['auc_macro'] = float(auc_macro)\n",
        "\n",
        "    ap_per_class = []\n",
        "    for c in range(targets.shape[1]):\n",
        "        if np.sum(targets[:, c]) == 0:\n",
        "            continue\n",
        "        ap = average_precision_score(targets[:, c], probs[:, c])\n",
        "        ap_per_class.append(ap)\n",
        "    metrics['mAP'] = float(np.mean(ap_per_class)) if len(ap_per_class) > 0 else np.nan\n",
        "\n",
        "    acc_micro = (targets == preds).mean()\n",
        "    metrics['acc_micro'] = float(acc_micro)\n",
        "\n",
        "    acc_per_class = []\n",
        "    for c in range(targets.shape[1]):\n",
        "        acc_c = (targets[:, c] == preds[:, c]).mean()\n",
        "        acc_per_class.append(acc_c)\n",
        "    metrics['acc_macro'] = float(np.mean(acc_per_class))\n",
        "\n",
        "    return metrics, probs, preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_DnZv9WiUma"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# 6. 공통 train/eval 루프 (supervised BCE)\n",
        "# =============================\n",
        "\n",
        "def train_one_epoch_supervised(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, targets in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def eval_supervised(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_logits = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, targets)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            all_logits.append(logits.cpu())\n",
        "            all_targets.append(targets.cpu())\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    all_logits = torch.cat(all_logits).numpy()\n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "    return avg_loss, all_logits, all_targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2WPwbdpiV4X"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# 7. Supervised Contrastive Loss (multi-label)\n",
        "# supCon: 두 샘플이 하나라도 같은 label을 공유하면 positive pair\n",
        "# =============================\n",
        "\n",
        "def supervised_contrastive_loss(features, labels, temperature=0.07):\n",
        "    \"\"\"\n",
        "    features: (B, d), normalized\n",
        "    labels: (B, C) multi-hot\n",
        "    \"\"\"\n",
        "    device = features.device\n",
        "    batch_size = features.shape[0]\n",
        "\n",
        "    # similarity matrix\n",
        "    sim_matrix = torch.matmul(features, features.T) / temperature  # (B,B)\n",
        "    # 제거: self-similarity\n",
        "    logits_mask = torch.ones_like(sim_matrix) - torch.eye(batch_size, device=device)\n",
        "    sim_matrix = sim_matrix * logits_mask\n",
        "\n",
        "    # positive mask: share at least one label\n",
        "    labels = labels.float()\n",
        "    # (B,B): entry (i,j) = 1 if share at least one label and i!=j\n",
        "    pos_mask = (torch.matmul(labels, labels.T) > 0).float() * logits_mask\n",
        "\n",
        "    # log-sum-exp over all j!=i\n",
        "    exp_sim = torch.exp(sim_matrix) * logits_mask\n",
        "    log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "    # for each i, average over positive j\n",
        "    pos_count = pos_mask.sum(dim=1)\n",
        "    # 피하기: positive가 없는 샘플은 contribution 0\n",
        "    mean_log_prob_pos = (pos_mask * log_prob).sum(dim=1) / (pos_count + 1e-8)\n",
        "\n",
        "    # loss = - 평균(mean over i of mean_log_prob_pos)\n",
        "    loss = -mean_log_prob_pos.mean()\n",
        "    return loss\n",
        "\n",
        "# =============================\n",
        "# 7-1. SupCon 학습 루프 (backbone freeze + proj head만 학습)\n",
        "# =============================\n",
        "\n",
        "def train_one_epoch_supcon(model, loader, optimizer, temperature=0.07):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, targets in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        z = model(images)  # normalized embeddings\n",
        "        loss = supervised_contrastive_loss(z, targets, temperature=temperature)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "# =============================\n",
        "# 7-2. SupCon으로 backbone+proj 고정한 후,\n",
        "#      같은 backbone의 feature를 사용해서 Linear classifier(BCE) 학습\n",
        "# =============================\n",
        "\n",
        "def extract_features(backbone, loader):\n",
        "    backbone.eval()\n",
        "    feats_list = []\n",
        "    labels_list = []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "            feats = backbone(images)\n",
        "            feats_list.append(feats.cpu())\n",
        "            labels_list.append(targets.cpu())\n",
        "    feats = torch.cat(feats_list)\n",
        "    labels = torch.cat(labels_list)\n",
        "    return feats, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pOUjrFAibLL"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# 8. 전체 실험 함수\n",
        "# =============================\n",
        "\n",
        "def run_supervised_experiment(model_name, num_epochs=10, lr=1e-3, save_dir=\"results\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    backbone, in_features = create_backbone(model_name, pretrained=True)\n",
        "    model = LinearHeadModel(backbone, in_features, num_classes).to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=lr)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_state = None\n",
        "\n",
        "    history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        t0 = time.time()\n",
        "        train_loss = train_one_epoch_supervised(model, train_loader, optimizer, criterion)\n",
        "        val_loss, _, _ = eval_supervised(model, val_loader, criterion)\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        history['epoch'].append(epoch)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        print(f\"[Supervised-{model_name}] Epoch {epoch}/{num_epochs} \"\n",
        "              f\"train={train_loss:.4f} val={val_loss:.4f} time={elapsed:.1f}s\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_state = model.state_dict().copy()\n",
        "\n",
        "    # 기록 및 모델 저장\n",
        "    hist_df = pd.DataFrame(history)\n",
        "    hist_df.to_csv(os.path.join(save_dir, f\"{model_name}_supervised_history.csv\"), index=False)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(hist_df['epoch'], hist_df['train_loss'], label='train')\n",
        "    plt.plot(hist_df['epoch'], hist_df['val_loss'], label='val')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"{model_name} Supervised Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, f\"{model_name}_supervised_loss.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    weight_path = os.path.join(save_dir, f\"{model_name}_supervised_best.pth\")\n",
        "    if best_state is not None:\n",
        "        torch.save(best_state, weight_path)\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # test 평가\n",
        "    test_loss, test_logits, test_targets = eval_supervised(model, test_loader, criterion)\n",
        "    metrics, probs, preds = compute_metrics(test_logits, test_targets)\n",
        "\n",
        "    print(f\"[Supervised-{model_name}] Test: F1_micro={metrics['f1_micro']:.4f}, \"\n",
        "          f\"F1_macro={metrics['f1_macro']:.4f}, AUC_macro={metrics['auc_macro']:.4f}, \"\n",
        "          f\"mAP={metrics['mAP']:.4f}\")\n",
        "\n",
        "    # metrics 저장\n",
        "    with open(os.path.join(save_dir, f\"{model_name}_supervised_metrics.json\"), \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    # classification report 저장\n",
        "    from sklearn.metrics import classification_report\n",
        "    report = classification_report(\n",
        "        test_targets, preds,\n",
        "        target_names=[idx2label[i] for i in range(num_classes)],\n",
        "        zero_division=0, output_dict=True\n",
        "    )\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df.to_csv(os.path.join(save_dir, f\"{model_name}_supervised_classification_report.csv\"))\n",
        "\n",
        "    return metrics, weight_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hg1pIf9ifBw"
      },
      "outputs": [],
      "source": [
        "def run_supcon_experiment(model_name, num_epochs_supcon=5, num_epochs_cls=5, lr=1e-3, save_dir=\"results\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # 1) backbone + projection head로 SupCon 학습\n",
        "    backbone, in_features = create_backbone(model_name, pretrained=True)\n",
        "    supcon_model = SupConModel(backbone, in_features, proj_dim=128).to(device)\n",
        "\n",
        "    # proj head만 학습 (backbone은 이미 requires_grad=False)\n",
        "    optimizer = torch.optim.Adam(supcon_model.proj.parameters(), lr=lr)\n",
        "\n",
        "    history_supcon = {'epoch': [], 'train_loss': []}\n",
        "    for epoch in range(1, num_epochs_supcon+1):\n",
        "        t0 = time.time()\n",
        "        train_loss = train_one_epoch_supcon(supcon_model, train_loader, optimizer, temperature=0.07)\n",
        "        elapsed = time.time() - t0\n",
        "        history_supcon['epoch'].append(epoch)\n",
        "        history_supcon['train_loss'].append(train_loss)\n",
        "\n",
        "        print(f\"[SupCon-{model_name}] Epoch {epoch}/{num_epochs_supcon} \"\n",
        "              f\"train_supcon_loss={train_loss:.4f} time={elapsed:.1f}s\")\n",
        "\n",
        "    hist_df = pd.DataFrame(history_supcon)\n",
        "    hist_df.to_csv(os.path.join(save_dir, f\"{model_name}_supcon_pretrain_history.csv\"), index=False)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(hist_df['epoch'], hist_df['train_loss'], label='supcon_train_loss')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"{model_name} SupCon Pretrain Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, f\"{model_name}_supcon_pretrain_loss.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 2) SupCon으로 학습된 backbone을 그대로 쓰고 (proj는 feature용),\n",
        "    #    Linear classifier를 따로 학습\n",
        "    #    여기서는 backbone 출력 feature 위에 classifier를 붙인다.\n",
        "    backbone_after = supcon_model.backbone  # 이미 SupCon에서 backbone은 freeze\n",
        "\n",
        "    cls_model = LinearHeadModel(backbone_after, in_features, num_classes).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer_cls = torch.optim.Adam(cls_model.classifier.parameters(), lr=lr)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_state = None\n",
        "    history_cls = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(1, num_epochs_cls+1):\n",
        "        t0 = time.time()\n",
        "        train_loss = train_one_epoch_supervised(cls_model, train_loader, optimizer_cls, criterion)\n",
        "        val_loss, _, _ = eval_supervised(cls_model, val_loader, criterion)\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        history_cls['epoch'].append(epoch)\n",
        "        history_cls['train_loss'].append(train_loss)\n",
        "        history_cls['val_loss'].append(val_loss)\n",
        "\n",
        "        print(f\"[SupCon-CLS-{model_name}] Epoch {epoch}/{num_epochs_cls} \"\n",
        "              f\"train={train_loss:.4f} val={val_loss:.4f} time={elapsed:.1f}s\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_state = cls_model.state_dict().copy()\n",
        "\n",
        "    hist_df2 = pd.DataFrame(history_cls)\n",
        "    hist_df2.to_csv(os.path.join(save_dir, f\"{model_name}_supcon_cls_history.csv\"), index=False)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(hist_df2['epoch'], hist_df2['train_loss'], label='train')\n",
        "    plt.plot(hist_df2['epoch'], hist_df2['val_loss'], label='val')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"{model_name} SupCon+CLS Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, f\"{model_name}_supcon_cls_loss.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    weight_path = os.path.join(save_dir, f\"{model_name}_supcon_best.pth\")\n",
        "    if best_state is not None:\n",
        "        torch.save(best_state, weight_path)\n",
        "        cls_model.load_state_dict(best_state)\n",
        "\n",
        "    # test 평가\n",
        "    test_loss, test_logits, test_targets = eval_supervised(cls_model, test_loader, criterion)\n",
        "    metrics, probs, preds = compute_metrics(test_logits, test_targets)\n",
        "\n",
        "    print(f\"[SupCon-{model_name}] Test: F1_micro={metrics['f1_micro']:.4f}, \"\n",
        "          f\"F1_macro={metrics['f1_macro']:.4f}, AUC_macro={metrics['auc_macro']:.4f}, \"\n",
        "          f\"mAP={metrics['mAP']:.4f}\")\n",
        "\n",
        "    with open(os.path.join(save_dir, f\"{model_name}_supcon_metrics.json\"), \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    report = classification_report(\n",
        "        test_targets, preds,\n",
        "        target_names=[idx2label[i] for i in range(num_classes)],\n",
        "        zero_division=0, output_dict=True\n",
        "    )\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df.to_csv(os.path.join(save_dir, f\"{model_name}_supcon_classification_report.csv\"))\n",
        "\n",
        "    return metrics, weight_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymmrWULqih56",
        "outputId": "e7d7feaa-0cd7-4d91-a34e-b5b1e4c02159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Supervised training for vit =====\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 330M/330M [00:02<00:00, 155MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Supervised-vit] Epoch 1/10 train=0.1968 val=0.1830 time=185.8s\n",
            "[Supervised-vit] Epoch 2/10 train=0.1806 val=0.1812 time=185.3s\n",
            "[Supervised-vit] Epoch 3/10 train=0.1777 val=0.1790 time=184.9s\n",
            "[Supervised-vit] Epoch 4/10 train=0.1762 val=0.1780 time=185.7s\n",
            "[Supervised-vit] Epoch 5/10 train=0.1749 val=0.1785 time=185.3s\n",
            "[Supervised-vit] Epoch 6/10 train=0.1741 val=0.1781 time=184.9s\n",
            "[Supervised-vit] Epoch 7/10 train=0.1732 val=0.1782 time=185.0s\n",
            "[Supervised-vit] Epoch 8/10 train=0.1726 val=0.1780 time=185.5s\n",
            "[Supervised-vit] Epoch 9/10 train=0.1721 val=0.1776 time=185.5s\n",
            "[Supervised-vit] Epoch 10/10 train=0.1714 val=0.1774 time=185.2s\n",
            "[Supervised-vit] Test: F1_micro=0.2784, F1_macro=0.0725, AUC_macro=0.7170, mAP=0.1882\n",
            "\n",
            "===== Supervised Contrastive training for vit =====\n",
            "[SupCon-vit] Epoch 1/10 train_supcon_loss=4.8182 time=137.9s\n",
            "[SupCon-vit] Epoch 2/10 train_supcon_loss=4.8049 time=138.3s\n",
            "[SupCon-vit] Epoch 3/10 train_supcon_loss=4.8027 time=138.5s\n",
            "[SupCon-vit] Epoch 4/10 train_supcon_loss=4.8033 time=138.5s\n",
            "[SupCon-vit] Epoch 5/10 train_supcon_loss=4.8032 time=138.5s\n",
            "[SupCon-vit] Epoch 6/10 train_supcon_loss=4.8032 time=138.2s\n",
            "[SupCon-vit] Epoch 7/10 train_supcon_loss=4.8010 time=138.4s\n",
            "[SupCon-vit] Epoch 8/10 train_supcon_loss=4.8003 time=139.0s\n",
            "[SupCon-vit] Epoch 9/10 train_supcon_loss=4.8014 time=138.3s\n",
            "[SupCon-vit] Epoch 10/10 train_supcon_loss=4.7976 time=138.4s\n",
            "[SupCon-CLS-vit] Epoch 1/10 train=0.1987 val=0.1831 time=184.5s\n",
            "[SupCon-CLS-vit] Epoch 2/10 train=0.1806 val=0.1806 time=184.9s\n",
            "[SupCon-CLS-vit] Epoch 3/10 train=0.1779 val=0.1793 time=184.9s\n",
            "[SupCon-CLS-vit] Epoch 4/10 train=0.1763 val=0.1783 time=185.6s\n",
            "[SupCon-CLS-vit] Epoch 5/10 train=0.1750 val=0.1789 time=185.9s\n",
            "[SupCon-CLS-vit] Epoch 6/10 train=0.1742 val=0.1778 time=185.0s\n",
            "[SupCon-CLS-vit] Epoch 7/10 train=0.1733 val=0.1787 time=184.3s\n",
            "[SupCon-CLS-vit] Epoch 8/10 train=0.1728 val=0.1776 time=184.6s\n",
            "[SupCon-CLS-vit] Epoch 9/10 train=0.1721 val=0.1774 time=185.1s\n",
            "[SupCon-CLS-vit] Epoch 10/10 train=0.1714 val=0.1776 time=185.0s\n",
            "[SupCon-vit] Test: F1_micro=0.2806, F1_macro=0.0740, AUC_macro=0.7153, mAP=0.1878\n",
            "\n",
            "===== Supervised training for resnet50 =====\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 189MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Supervised-resnet50] Epoch 1/10 train=0.1987 val=0.1871 time=183.8s\n",
            "[Supervised-resnet50] Epoch 2/10 train=0.1862 val=0.1851 time=183.6s\n",
            "[Supervised-resnet50] Epoch 3/10 train=0.1841 val=0.1849 time=183.0s\n",
            "[Supervised-resnet50] Epoch 4/10 train=0.1830 val=0.1856 time=183.2s\n",
            "[Supervised-resnet50] Epoch 5/10 train=0.1821 val=0.1836 time=182.8s\n",
            "[Supervised-resnet50] Epoch 6/10 train=0.1817 val=0.1848 time=183.3s\n",
            "[Supervised-resnet50] Epoch 7/10 train=0.1808 val=0.1844 time=184.4s\n",
            "[Supervised-resnet50] Epoch 8/10 train=0.1805 val=0.1840 time=184.2s\n",
            "[Supervised-resnet50] Epoch 9/10 train=0.1794 val=0.1853 time=183.3s\n",
            "[Supervised-resnet50] Epoch 10/10 train=0.1791 val=0.1840 time=183.9s\n",
            "[Supervised-resnet50] Test: F1_micro=0.1970, F1_macro=0.0474, AUC_macro=0.6783, mAP=0.1632\n",
            "\n",
            "===== Supervised Contrastive training for resnet50 =====\n",
            "[SupCon-resnet50] Epoch 1/10 train_supcon_loss=4.8179 time=137.3s\n",
            "[SupCon-resnet50] Epoch 2/10 train_supcon_loss=4.8126 time=137.0s\n",
            "[SupCon-resnet50] Epoch 3/10 train_supcon_loss=4.8094 time=137.1s\n",
            "[SupCon-resnet50] Epoch 4/10 train_supcon_loss=4.8080 time=137.0s\n",
            "[SupCon-resnet50] Epoch 5/10 train_supcon_loss=4.8075 time=137.3s\n",
            "[SupCon-resnet50] Epoch 6/10 train_supcon_loss=4.8049 time=137.3s\n",
            "[SupCon-resnet50] Epoch 7/10 train_supcon_loss=4.8050 time=137.5s\n",
            "[SupCon-resnet50] Epoch 8/10 train_supcon_loss=4.8084 time=137.7s\n",
            "[SupCon-resnet50] Epoch 9/10 train_supcon_loss=4.8076 time=138.0s\n",
            "[SupCon-resnet50] Epoch 10/10 train_supcon_loss=4.8076 time=138.3s\n",
            "[SupCon-CLS-resnet50] Epoch 1/10 train=0.2010 val=0.1900 time=184.6s\n",
            "[SupCon-CLS-resnet50] Epoch 2/10 train=0.1867 val=0.1855 time=184.3s\n",
            "[SupCon-CLS-resnet50] Epoch 3/10 train=0.1841 val=0.1844 time=184.3s\n",
            "[SupCon-CLS-resnet50] Epoch 4/10 train=0.1833 val=0.1850 time=183.4s\n",
            "[SupCon-CLS-resnet50] Epoch 5/10 train=0.1815 val=0.1832 time=183.3s\n",
            "[SupCon-CLS-resnet50] Epoch 6/10 train=0.1815 val=0.1827 time=183.2s\n",
            "[SupCon-CLS-resnet50] Epoch 7/10 train=0.1808 val=0.1848 time=184.6s\n",
            "[SupCon-CLS-resnet50] Epoch 8/10 train=0.1804 val=0.1832 time=184.8s\n",
            "[SupCon-CLS-resnet50] Epoch 9/10 train=0.1800 val=0.1853 time=184.0s\n",
            "[SupCon-CLS-resnet50] Epoch 10/10 train=0.1794 val=0.1826 time=184.3s\n",
            "[SupCon-resnet50] Test: F1_micro=0.2495, F1_macro=0.0487, AUC_macro=0.6768, mAP=0.1622\n",
            "\n",
            "===== Supervised training for efficientnet =====\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 129MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Supervised-efficientnet] Epoch 1/10 train=0.2207 val=0.1953 time=184.4s\n",
            "[Supervised-efficientnet] Epoch 2/10 train=0.1971 val=0.1909 time=183.7s\n",
            "[Supervised-efficientnet] Epoch 3/10 train=0.1928 val=0.1889 time=182.7s\n",
            "[Supervised-efficientnet] Epoch 4/10 train=0.1898 val=0.1877 time=183.1s\n",
            "[Supervised-efficientnet] Epoch 5/10 train=0.1880 val=0.1876 time=183.0s\n",
            "[Supervised-efficientnet] Epoch 6/10 train=0.1864 val=0.1867 time=182.5s\n",
            "[Supervised-efficientnet] Epoch 7/10 train=0.1844 val=0.1862 time=182.7s\n",
            "[Supervised-efficientnet] Epoch 8/10 train=0.1830 val=0.1860 time=183.1s\n",
            "[Supervised-efficientnet] Epoch 9/10 train=0.1827 val=0.1863 time=182.4s\n",
            "[Supervised-efficientnet] Epoch 10/10 train=0.1818 val=0.1867 time=182.7s\n",
            "[Supervised-efficientnet] Test: F1_micro=0.2613, F1_macro=0.0521, AUC_macro=0.6494, mAP=0.1550\n",
            "\n",
            "===== Supervised Contrastive training for efficientnet =====\n",
            "[SupCon-efficientnet] Epoch 1/10 train_supcon_loss=4.8389 time=137.8s\n",
            "[SupCon-efficientnet] Epoch 2/10 train_supcon_loss=4.8175 time=137.8s\n",
            "[SupCon-efficientnet] Epoch 3/10 train_supcon_loss=4.8145 time=137.0s\n",
            "[SupCon-efficientnet] Epoch 4/10 train_supcon_loss=4.8118 time=137.3s\n",
            "[SupCon-efficientnet] Epoch 5/10 train_supcon_loss=4.8128 time=137.6s\n",
            "[SupCon-efficientnet] Epoch 6/10 train_supcon_loss=4.8104 time=137.8s\n",
            "[SupCon-efficientnet] Epoch 7/10 train_supcon_loss=4.8100 time=138.3s\n",
            "[SupCon-efficientnet] Epoch 8/10 train_supcon_loss=4.8141 time=138.0s\n",
            "[SupCon-efficientnet] Epoch 9/10 train_supcon_loss=4.8110 time=138.3s\n",
            "[SupCon-efficientnet] Epoch 10/10 train_supcon_loss=4.8073 time=137.7s\n",
            "[SupCon-CLS-efficientnet] Epoch 1/10 train=0.2192 val=0.1951 time=183.4s\n",
            "[SupCon-CLS-efficientnet] Epoch 2/10 train=0.1964 val=0.1908 time=183.0s\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# 9. 전체 모델에 대해 실험 실행\n",
        "# =============================\n",
        "\n",
        "save_dir = \"results_nih_chest\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "supervised_results = {}\n",
        "supcon_results = {}\n",
        "\n",
        "for name in ['vit', 'resnet50', 'efficientnet']:\n",
        "    print(f\"\\n===== Supervised training for {name} =====\")\n",
        "    sup_metrics, sup_wpath = run_supervised_experiment(\n",
        "        name, num_epochs=10, lr=1e-3, save_dir=save_dir\n",
        "    )\n",
        "    supervised_results[name] = sup_metrics\n",
        "\n",
        "    print(f\"\\n===== Supervised Contrastive training for {name} =====\")\n",
        "    sc_metrics, sc_wpath = run_supcon_experiment(\n",
        "        name, num_epochs_supcon=10, num_epochs_cls=10, lr=1e-3, save_dir=save_dir\n",
        "    )\n",
        "    supcon_results[name] = sc_metrics\n",
        "\n",
        "# 결과 비교 테이블\n",
        "supervised_df = pd.DataFrame(supervised_results).T\n",
        "supcon_df     = pd.DataFrame(supcon_results).T\n",
        "\n",
        "supervised_df.to_csv(os.path.join(save_dir, \"supervised_model_comparison.csv\"))\n",
        "supcon_df.to_csv(os.path.join(save_dir, \"supcon_model_comparison.csv\"))\n",
        "\n",
        "print(\"\\nSupervised results:\")\n",
        "display(supervised_df)\n",
        "\n",
        "print(\"\\nSupCon results:\")\n",
        "display(supcon_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO_4VCUtnQ0U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}